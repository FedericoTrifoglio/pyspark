{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding RDD\n",
    "\n",
    "Inspired by https://campus.datacamp.com/courses/big-data-fundamentals-with-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import os\n",
    "from utils.stopwords import stopwords\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "# sc.stop()\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SparkContext`\n",
    "\n",
    "Main **entry point** for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be **used to create RDD** and broadcast variables on that cluster.\n",
    "\n",
    "`SparkSession`\n",
    "\n",
    "The **entry point** to programming Spark with the Dataset and DataFrame API. A SparkSession can be **used to create DataFrame**, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LON-10-L-770.pc.london.mintel.ad:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sc.textFile` reads a text file and return it as an RDD of Strings. Another way to create an RRD object is `.parallelize()`.\n",
    "\n",
    "**Resilient Distributed Dataset** or **RDD** in a PySpark is a core data structure of PySpark. PySpark RDDâ€™s is a low-level object and are highly efficient in performing distributed tasks.\n",
    "\n",
    "PySpark RDD has a set of operations to accomplish any task. These operations are of two types:\n",
    "\n",
    "1. Transformations\n",
    "\n",
    "2. Actions\n",
    "\n",
    "**Transformations** are a kind of operation that takes an RDD as input and produces another RDD as output. After applying the transformation, it creates a **Directed Acyclic Graph** or **DAG** for computations and ends after applying any actions on it. This is the reason they are called **lazy evaluation** processes.\n",
    "\n",
    "Some popular transformations are:\n",
    "- `.map`\n",
    "- `.filter`\n",
    "\n",
    "Other popular transformations: https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\n",
    "\n",
    "\n",
    "**Actions** are a kind of operation which are applied on an RDD to produce a single value. These methods are applied on a resultant RDD and produces a non-RDD value, thus removing the laziness of the transformation of RDD.\n",
    "\n",
    "Some popular actions are:\n",
    "- `.collect`\n",
    "- `.take`\n",
    "- `.count`\n",
    "\n",
    "Other popular actions: https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\n",
    "\n",
    "Why is `.reduce` an action, but `.reduceByKey` is a transformation?\n",
    "\n",
    "`.reduce` must pull the entire dataset down into a single location because it is reducing to one final value. `.reduceByKey` on the other hand is one value for each key. And since this action can be run on each machine locally first then it can remain an RDD and have further transformations done on its dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example: Shakespeare's most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the RRD: 961947\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data//Complete_Shakespeare.txt\"\n",
    "\n",
    "# Create the RRD from a text file\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Transform to RRD of single words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "print(\"Total number of words in the RRD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou,4512\n",
      "thy,3915\n",
      "shall,3247\n",
      "good,2168\n",
      "would,2134\n",
      "Enter,2079\n",
      "thee,1895\n",
      "hath,1720\n",
      "like,1646\n",
      "you,,1581\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words (after converting to lower case)\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stopwords)\n",
    "\n",
    "# Create a tuple for each word containing the word as key and 1 as value\n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Reduce by key to count the occurrences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Swap the keys and values so we can sort by key next\n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "\tprint(\"{},{}\". format(word[1], word[0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a516266561e31a98e1f505056415c7e921614f94151a9aed0f32dbb4f04bbe25"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
