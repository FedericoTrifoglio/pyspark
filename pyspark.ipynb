{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark\n",
    "\n",
    "Inspired by https://app.datacamp.com/learn/courses/introduction-to-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000019AD6CC2B20>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# spark.stop()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LON-10-L-770.pc.london.mintel.ad:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of the `SparkContext` as a connection to a cluster and the `SparkSession` as the interface with that connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add('year', IntegerType(), True) \\\n",
    "      .add('month', IntegerType(), True) \\\n",
    "      .add('day', IntegerType(), True) \\\n",
    "      .add('dep_time', IntegerType(), True) \\\n",
    "      .add('dep_delay', IntegerType(), True) \\\n",
    "      .add('arr_time', IntegerType(), True) \\\n",
    "      .add('arr_delay', IntegerType(), True) \\\n",
    "      .add('carrier', StringType(), True) \\\n",
    "      .add('tailnum', StringType(), True) \\\n",
    "      .add('flight', StringType(), True) \\\n",
    "      .add('origin', StringType(), True) \\\n",
    "      .add('dest', StringType(), True) \\\n",
    "      .add('air_time', IntegerType(), True) \\\n",
    "      .add('distance', IntegerType(), True) \\\n",
    "      .add('hour', IntegerType(), True) \\\n",
    "      .add('minute', IntegerType(), True) \\\n",
    "\n",
    "flights = spark.read.options(header='True') \\\n",
    "                    .schema(schema) \\\n",
    "                    .csv('data//flights_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flights` is stored locally, not in the `SparkSession` catalog. This means that we can use all the Spark DataFrame methods on it, but we can't access the data in other contexts.\n",
    "\n",
    "For example, a SQL query (using the `.sql()` method) that references your DataFrame will throw an error. To access the data in this way, we have to save it as a temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.createOrReplaceTempView('flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run a query, make the dataset smaller and convert it to a pandas data frame (eg for EDA, plotting).\n",
    "\n",
    "`--sql` enables SQL syntax highlighting in VSCode.\n",
    "https://github.com/ptweir/python-string-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"--sql\n",
    "SELECT origin, dest, COUNT(*) as N \n",
    "FROM flights \n",
    "GROUP BY origin, dest\n",
    ";\n",
    "\"\"\"\n",
    "flight_counts = spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd_counts = flight_counts.toPandas()\n",
    "pd_counts.hist('N')\n",
    "plt.title(\"Histogram of flights for each origin-destination pair\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL-equivalent PySpark DataFrames methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------------------+\n",
      "|origin|dest|      duration_hrs|\n",
      "+------+----+------------------+\n",
      "|   SEA| LAX|               2.2|\n",
      "|   SEA| HNL|               6.0|\n",
      "|   SEA| SFO|              1.85|\n",
      "|   PDX| SJC|1.3833333333333333|\n",
      "|   SEA| BUR|2.1166666666666667|\n",
      "+------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT origin, dest, air_time/60 AS duration_hrs\n",
    "FROM flights\n",
    ";\n",
    "\"\"\"\n",
    "spark.sql(q).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be achieved with `.select` and `.alias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------------------+\n",
      "|origin|dest|      duration_hrs|\n",
      "+------+----+------------------+\n",
      "|   SEA| LAX|               2.2|\n",
      "|   SEA| HNL|               6.0|\n",
      "|   SEA| SFO|              1.85|\n",
      "|   PDX| SJC|1.3833333333333333|\n",
      "|   SEA| BUR|2.1166666666666667|\n",
      "+------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights \\\n",
    "    .select('origin', 'dest', (flights.air_time/60).alias('duration_hrs')) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be achieved with `.filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+\n",
      "|origin|dest|distance|\n",
      "+------+----+--------+\n",
      "|   SEA| SAN|    1050|\n",
      "|   SEA| ORD|    1721|\n",
      "|   SEA| PHX|    1107|\n",
      "|   SEA| ANC|    1448|\n",
      "|   SEA| MDW|    1733|\n",
      "+------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT origin, dest, distance\n",
    "FROM flights\n",
    "WHERE distance > 1000 AND distance < 2000\n",
    ";\n",
    "\"\"\"\n",
    "spark.sql(q).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+\n",
      "|origin|dest|distance|\n",
      "+------+----+--------+\n",
      "|   SEA| SAN|    1050|\n",
      "|   SEA| ORD|    1721|\n",
      "|   SEA| PHX|    1107|\n",
      "|   SEA| ANC|    1448|\n",
      "|   SEA| MDW|    1733|\n",
      "+------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights \\\n",
    "    .filter(flights.distance > 1000) \\\n",
    "    .filter(flights.distance < 2000) \\\n",
    "    .select('origin', 'dest', 'distance') \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|min_dist_from_PDX|\n",
      "+-----------------+\n",
      "|              106|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT MIN(distance) AS min_dist_from_PDX\n",
    "FROM flights\n",
    "WHERE origin = \"PDX\"\n",
    ";\n",
    "\"\"\"\n",
    "spark.sql(q).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be achieved with `.min`, `.max`, `.count`, `.avg` etc but these methods are only available for a GroupedData object, so we need to convert it with `.groupBy()` even if we don't actually use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|min(distance)|\n",
      "+-------------+\n",
      "|          106|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights \\\n",
    "    .filter(flights.origin=='PDX') \\\n",
    "    .groupBy() \\\n",
    "    .min('distance') \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantage is that we cannot alias the new column. The `.agg` method accepts sql functions like `min()`, `max()`, `count()`, `avg()` and they can be aliased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|min_dist_from_PDX|\n",
      "+-----------------+\n",
      "|              106|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "\n",
    "flights \\\n",
    "    .filter(flights.origin=='PDX') \\\n",
    "    .groupBy() \\\n",
    "    .agg(min('distance').alias('min_dist_from_PDX')) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-----------------+\n",
      "|dest|     avg_dep_delay|     sd_dep_delay|\n",
      "+----+------------------+-----------------+\n",
      "| CLE|              31.5|23.33452377915607|\n",
      "| BWI|              16.0|32.11343530786962|\n",
      "| SFO|13.309370988446727| 34.1416307929602|\n",
      "| EWR|12.753521126760564|34.52046091170115|\n",
      "| MCO|11.096774193548388|40.20394245900907|\n",
      "+----+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, stddev\n",
    "\n",
    "flights \\\n",
    "    .groupBy('dest') \\\n",
    "    .agg(avg('dep_delay').alias('avg_dep_delay'), stddev('dep_delay').alias('sd_dep_delay')) \\\n",
    "    .orderBy('avg_dep_delay', ascending=False) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's improve the table above by adding a column with the actual names of the airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add('faa', StringType(), True) \\\n",
    "      .add('name', StringType(), True) \\\n",
    "      .add('lat', DoubleType(), True) \\\n",
    "      .add('lon', DoubleType(), True) \\\n",
    "      .add('alt', IntegerType(), True) \\\n",
    "      .add('tz', IntegerType(), True) \\\n",
    "      .add('dst', StringType(), True) \\\n",
    "\n",
    "airports = spark.read.options(header='True') \\\n",
    "                     .schema(schema) \\\n",
    "                     .csv('data//airports.csv')\n",
    "\n",
    "airports.createOrReplaceTempView('airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SQL we would do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+------------------+-----------------+\n",
      "|dest_code|dest_name                |avg_dep_delay     |sd_dep_delay     |\n",
      "+---------+-------------------------+------------------+-----------------+\n",
      "|CLE      |Cleveland Hopkins Intl   |31.5              |23.33452377915607|\n",
      "|BWI      |Baltimore Washington Intl|16.0              |32.11343530786962|\n",
      "|SFO      |San Francisco Intl       |13.309370988446727|34.1416307929602 |\n",
      "|EWR      |Newark Liberty Intl      |12.753521126760564|34.52046091170115|\n",
      "|MCO      |Orlando Intl             |11.096774193548388|40.20394245900907|\n",
      "+---------+-------------------------+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT dest AS dest_code, name as dest_name, avg_dep_delay, sd_dep_delay\n",
    "FROM (\n",
    "    SELECT dest, AVG(dep_delay) AS avg_dep_delay, STDDEV(dep_delay) AS sd_dep_delay\n",
    "    FROM flights\n",
    "    GROUP BY dest\n",
    ")\n",
    "JOIN airports ON dest = faa\n",
    "ORDER BY avg_dep_delay DESC\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(q).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PySpark API the same can be achieved with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+------------------+-----------------+\n",
      "|dest_code|dest_name                |avg_dep_delay     |sd_dep_delay     |\n",
      "+---------+-------------------------+------------------+-----------------+\n",
      "|CLE      |Cleveland Hopkins Intl   |31.5              |23.33452377915607|\n",
      "|BWI      |Baltimore Washington Intl|16.0              |32.11343530786962|\n",
      "|SFO      |San Francisco Intl       |13.309370988446727|34.1416307929602 |\n",
      "|EWR      |Newark Liberty Intl      |12.753521126760564|34.52046091170115|\n",
      "|MCO      |Orlando Intl             |11.096774193548388|40.20394245900907|\n",
      "+---------+-------------------------+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights \\\n",
    "    .groupBy('dest') \\\n",
    "    .agg(avg('dep_delay').alias('avg_dep_delay'), stddev('dep_delay').alias('sd_dep_delay')) \\\n",
    "    .orderBy('avg_dep_delay', ascending=False) \\\n",
    "    .join(airports, flights.dest == airports.faa, 'left') \\\n",
    "    .select((flights.dest).alias('dest_code'), (airports.name).alias('dest_name'), 'avg_dep_delay', 'sd_dep_delay') \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add('tailnum', StringType(), True) \\\n",
    "      .add('year', IntegerType(), True) \\\n",
    "      .add('type', StringType(), True) \\\n",
    "      .add('manufacturer', StringType(), True) \\\n",
    "      .add('model', StringType(), True) \\\n",
    "      .add('engines', IntegerType(), True) \\\n",
    "      .add('seats', IntegerType(), True) \\\n",
    "      .add('speed', IntegerType(), True) \\\n",
    "      .add('engine', StringType(), True) \\\n",
    "\n",
    "planes = spark.read.options(header='True') \\\n",
    "                    .schema(schema) \\\n",
    "                    .csv('data//planes.csv')\n",
    "\n",
    "planes.createOrReplaceTempView('planes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = planes.withColumnRenamed('year', 'plane_year')\n",
    "flights_planes = flights.join(planes, on='tailnum', how='left')\n",
    "flights_planes.createOrReplaceTempView('flights_planes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT month,\n",
    "       air_time,\n",
    "       carrier, \n",
    "       dest, \n",
    "       year - plane_year AS plane_age,\n",
    "       CASE WHEN arr_delay > 0 THEN 1 ELSE 0 END AS label\n",
    "FROM flights_planes\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "model_data = spark.sql(q)\n",
    "model_data.createOrReplaceTempView('model_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n"
     ]
    }
   ],
   "source": [
    "print((model_data.count(), len(model_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+----+---------+-----+\n",
      "|month|air_time|carrier|dest|plane_age|label|\n",
      "+-----+--------+-------+----+---------+-----+\n",
      "|    0|      75|      0|   0|      646|    0|\n",
      "+-----+--------+-------+----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT COUNT(*) - COUNT(month) AS month,\n",
    "       COUNT(*) - COUNT(air_time) AS air_time,\n",
    "       COUNT(*) - COUNT(carrier) AS carrier,\n",
    "       COUNT(*) - COUNT(dest) AS dest,\n",
    "       COUNT(*) - COUNT(plane_age) AS plane_age,\n",
    "       COUNT(*) - COUNT(label) AS label\n",
    "FROM (\n",
    "    SELECT month,\n",
    "           air_time,\n",
    "           carrier, \n",
    "           dest, \n",
    "           year - plane_year AS plane_age,\n",
    "           CASE WHEN arr_delay > 0 THEN 1 ELSE 0 END AS label\n",
    "    FROM flights_planes\n",
    ")\n",
    ";\n",
    "\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be achieved with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+----+---------+-----+\n",
      "|month|air_time|carrier|dest|plane_age|label|\n",
      "+-----+--------+-------+----+---------+-----+\n",
      "|    0|      75|      0|   0|      646|    0|\n",
      "+-----+--------+-------+----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "model_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in model_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|row_count_one_or_more_null|\n",
      "+--------------------------+\n",
      "|                       697|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT COUNT(*) AS row_count_one_or_more_null\n",
    "FROM (\n",
    "    SELECT month,\n",
    "           air_time,\n",
    "           carrier, \n",
    "           dest, \n",
    "           year - plane_year AS plane_age,\n",
    "           CASE WHEN arr_delay > 0 THEN 1 ELSE 0 END AS label\n",
    "    FROM flights_planes\n",
    ")\n",
    "WHERE month IS NULL OR air_time IS NULL OR carrier IS NULL OR dest IS NULL OR plane_age IS NULL OR label IS NULL\n",
    ";\n",
    "\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be achieved with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "697"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.filter(\" OR \".join([c + ' IS NULL' for c in model_data.columns])).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9303, 6)\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT month,\n",
    "           air_time,\n",
    "           carrier, \n",
    "           dest, \n",
    "           year - plane_year AS plane_age,\n",
    "           CASE WHEN arr_delay > 0 THEN 1 ELSE 0 END AS label\n",
    "    FROM flights_planes\n",
    ")\n",
    "WHERE month IS NOT NULL AND air_time IS NOT NULL AND carrier IS NOT NULL AND dest IS NOT NULL AND plane_age IS NOT NULL AND label IS NOT NULL\n",
    ";\n",
    "\"\"\"\n",
    "model_data_nona = spark.sql(q)\n",
    "print((model_data_nona.count(), len(model_data_nona.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_nona.createOrReplaceTempView('model_data_nona')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be achieved with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9303, 6)\n"
     ]
    }
   ],
   "source": [
    "model_data_nona = model_data.dropna()\n",
    "print((model_data_nona.count(), len(model_data_nona.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|     prop_no_delay|         prop_delay|\n",
      "+------------------+-------------------+\n",
      "|0.6242072449747393|0.37579275502526066|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT 1-AVG(label) AS prop_no_delay, AVG(label) AS prop_delay\n",
    "FROM model_data_nona\n",
    ";\n",
    "\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be achieved with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|     prop_no_delay|         prop_delay|\n",
      "+------------------+-------------------+\n",
      "|0.6242072449747393|0.37579275502526066|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_data_nona.agg((1-avg('label')).alias('prop_no_delay'), avg('label').alias('prop_delay')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build a preprocessing pipeline to one-hot encode the categorical features and merge them back with the numerical features\n",
    "2. Split the processed data into train and test\n",
    "3. Build hyperparameter grid for the regularization parameter and the L1/L2 ratio (Lasso or Ridge) for a logistic regression model\n",
    "4. Train the model using 5-fold cross validation and evaluate using ROC AUC\n",
    "5. Evaluate the best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "carr_indexer = StringIndexer(inputCol='carrier', outputCol='carrier_index')\n",
    "carr_encoder = OneHotEncoder(inputCol='carrier_index', outputCol='carrier_fact')\n",
    "\n",
    "dest_indexer = StringIndexer(inputCol='dest', outputCol='dest_index')\n",
    "dest_encoder = OneHotEncoder(inputCol='dest_index', outputCol='dest_fact')\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=['month', 'air_time', 'carrier_fact', 'dest_fact', 'plane_age'], outputCol='features')\n",
    "\n",
    "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_data = flights_pipe.fit(model_data_nona).transform(model_data_nona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+----+---------+-----+----------+---------------+-------------+--------------+--------------------+\n",
      "|month|air_time|carrier|dest|plane_age|label|dest_index|      dest_fact|carrier_index|  carrier_fact|            features|\n",
      "+-----+--------+-------+----+---------+-----+----------+---------------+-------------+--------------+--------------------+\n",
      "|   12|     132|     VX| LAX|        3|    0|       1.0| (68,[1],[1.0])|          7.0|(10,[7],[1.0])|(81,[0,1,9,13,80]...|\n",
      "|    1|     360|     AS| HNL|        8|    1|      19.0|(68,[19],[1.0])|          0.0|(10,[0],[1.0])|(81,[0,1,2,31,80]...|\n",
      "|    3|     111|     VX| SFO|        3|    1|       0.0| (68,[0],[1.0])|          7.0|(10,[7],[1.0])|(81,[0,1,9,12,80]...|\n",
      "|    4|      83|     WN| SJC|       22|    1|       7.0| (68,[7],[1.0])|          1.0|(10,[1],[1.0])|(81,[0,1,3,19,80]...|\n",
      "|    3|     127|     AS| BUR|       15|    1|      22.0|(68,[22],[1.0])|          0.0|(10,[0],[1.0])|(81,[0,1,2,34,80]...|\n",
      "+-----+--------+-------+----+---------+-----+----------+---------------+-------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "piped_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = piped_data.randomSplit([.6,.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as evals\n",
    "import pyspark.ml.tuning as tune\n",
    "import numpy as np\n",
    "\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "\n",
    "grid = tune.ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, np.arange(0, .1, .01)) \\\n",
    "    .addGrid(lr.elasticNetParam, [0,1]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator,\n",
    "               numFolds=5\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid=LogisticRegression_267b260b7440, numClasses=2, numFeatures=81\n"
     ]
    }
   ],
   "source": [
    "best_lr = lr.fit(training)\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6962808496360657\n"
     ]
    }
   ],
   "source": [
    "test_results = best_lr.transform(test)\n",
    "print(evaluator.evaluate(test_results))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a516266561e31a98e1f505056415c7e921614f94151a9aed0f32dbb4f04bbe25"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
